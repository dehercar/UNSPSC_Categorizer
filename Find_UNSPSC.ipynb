{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DescriptionParser, Sorting_Hat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_filename = 'Cleaning_Script_Template.xlsx'\n",
    "df = pd.read_excel(original_filename,sheet_name='Descriptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\David/nltk_data'\n    - 'c:\\\\Users\\\\David\\\\anaconda3\\\\envs\\\\myenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\David\\\\anaconda3\\\\envs\\\\myenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\David\\\\anaconda3\\\\envs\\\\myenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\David\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m DescriptionParser\u001b[39m.\u001b[39;49mrun_clean_dataset(df,desc_orig\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mDescription_Original\u001b[39;49m\u001b[39m'\u001b[39;49m,desc_output\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdesc_clean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m df \u001b[39m=\u001b[39m Sorting_Hat\u001b[39m.\u001b[39mrun_unspsc(df,desc_clean\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdesc_clean\u001b[39m\u001b[39m'\u001b[39m,predicted_col_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munspsc_pred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mcleaned_data.csv\u001b[39m\u001b[39m'\u001b[39m,encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8-sig\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\David\\python_codes\\UNSPSC_Categorizer\\DescriptionParser.py:183\u001b[0m, in \u001b[0;36mrun_clean_dataset\u001b[1;34m(df, desc_orig, desc_output)\u001b[0m\n\u001b[0;32m    181\u001b[0m final_words \u001b[39m=\u001b[39m []\n\u001b[0;32m    182\u001b[0m word_lemmatized \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m--> 183\u001b[0m pos_tagged \u001b[39m=\u001b[39m pos_tag(word_tokenize(description))\n\u001b[0;32m    184\u001b[0m wordnet_tagged \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: (x[\u001b[39m0\u001b[39m], pos_tagger(x[\u001b[39m1\u001b[39m])), pos_tagged))\n\u001b[0;32m    185\u001b[0m \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m wordnet_tagged:\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\David/nltk_data'\n    - 'c:\\\\Users\\\\David\\\\anaconda3\\\\envs\\\\myenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\David\\\\anaconda3\\\\envs\\\\myenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\David\\\\anaconda3\\\\envs\\\\myenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\David\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df = DescriptionParser.run_clean_dataset(df,desc_orig='Description_Original',desc_output='desc_clean')\n",
    "df = Sorting_Hat.run_unspsc(df,desc_clean='desc_clean',predicted_col_name='unspsc_pred')\n",
    "df.to_csv('cleaned_data.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
